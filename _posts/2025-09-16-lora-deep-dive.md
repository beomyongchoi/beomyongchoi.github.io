### [최종 수정 제안] LoRA (Low-Rank Adaptation)에 대한 고찰

최근 LLM의 파라미터 효율적 파인튜닝(PEFT)이 중요한 화두로 떠오르면서, 다양한 기법 중 LoRA(Low-Rank Adaptation)에 대해 깊이 살펴볼 기회가 있었습니다. LoRA는 파인튜닝의 높은 비용 문제를 해결하는 매우 실용적이면서도 독창적인 접근법을 제시합니다.

이 글에서는 LoRA의 핵심 아이디어부터 동작 원리, 그리고 실용적인 장단점까지 제가 이해한 내용을 바탕으로 정리해보고자 합니다.

---

#### 1. 문제의 시작: 전체 파인튜닝(Full Fine-tuning)의 비용

LLM을 특정 도메인이나 과제에 맞게 파인튜닝하는 가장 직관적인 방법은 '전체 파인튜닝'입니다. 이는 사전 학습된 모델(Pre-trained Model)의 모든 가중치(`W`)를 새로운 데이터에 맞게 업데이트하여, `W' = W + ΔW` 라는 새로운 모델을 만드는 과정입니다.

하지만 이 방식은 수천억 개에 달하는 파라미터 전체를 학습시켜야 하므로 막대한 GPU 메모리와 시간을 요구합니다. 또한, 새로운 과제 하나마다 원본에 버금가는 크기의 모델 전체를 새로 저장해야 하는 명백한 비효율이 발생합니다.

---

#### 2. LoRA의 핵심 가설: "변화량(ΔW)은 Low-Rank 특성을 띤다"

LoRA의 저자들은 여기서 중요한 가설을 제시합니다.

> **[논문 원문 (Abstract):](https://arxiv.org/pdf/2106.09685)**
> "We hypothesize that the change in weights during model adaptation has a low “intrinsic rank”, leading to our proposed method."

> **한국어 번역 및 해설:**
> "우리의 가설은, 모델을 (새로운 과제에) 적응시키는 동안 발생하는 가중치의 변화량은 낮은 **'내재적 계급(intrinsic rank)'**을 가질 것이라는 점이며, 이는 우리가 제안한 방법으로 이어짐."

여기서 'Rank가 낮다'는 것은, 행렬이 담고 있는 정보의 복잡도가 높지 않다는 의미입니다. 즉, 거대한 `ΔW` 행렬이 실제로는 **두 개의 훨씬 작고 차원이 낮은 행렬 `A`와 `B`의 곱(`ΔW ≈ B * A`)으로 근사될 수 있다**는 것입니다. 모델을 특정 과제에 맞게 조정하는 데 필요한 정보는, 생각보다 복잡하지 않으며 훨씬 단순한 형태로 압축될 수 있다는 아이디어입니다.

---

#### 3. LoRA의 아키텍처: 두 개의 경로를 통한 학습

이 가설을 바탕으로, LoRA는 기존 가중치 `W`는 그대로 둔 채(frozen), `ΔW`를 모사하는 작은 행렬 `A`와 `B`만 학습시킵니다.

![lora](/public/assets/lora.png)

위 그림처럼 LoRA의 학습은 두 개의 경로로 나뉩니다.

1.  **기존 경로**: 입력 `x`가 원래의 거대한 `W`를 통과합니다. (`h = Wx`)
2.  **LoRA 경로**: 동일한 입력 `x`가 저차원 행렬 `A`와 `B`를 차례로 통과합니다. (`Δh = BAx`)

최종 출력은 이 두 경로의 결과를 합산한 `h + Δh`가 됩니다. 이는 **원본 모델의 풍부한 표현력은 그대로 활용하면서, 새로운 과제에 필요한 최소한의 '보정값'만 효율적으로 학습하여 더해주는** 구조입니다.

이러한 설계 덕분에 학습 대상 파라미터가 극적으로 줄어듭니다. 예를 들어 4096x4096 행렬(약 1,677만 파라미터)을 튜닝할 때, LoRA(rank=8)를 적용하면 `(4096*8) + (8*4096)`인 약 6만 5천 개의 파라미터만 학습하면 되므로, **99% 이상의 파라미터 절감 효과**를 얻을 수 있습니다.

---

#### 4. 직관적인 이해: 포토샵 플러그인 비유

이 과정을 '포토샵'에 비유하면 더 쉽게 이해할 수 있습니다.

1.  **Pre-trained Model -> 포토샵 원본 프로그램**
    -   `klue/bert-base`와 같은 거대 언어 모델은, 수많은 기능이 담긴 포토샵 프로그램과 같습니다. 그 자체로 이미지 처리(언어 이해)에 대한 엄청난 잠재력을 갖추고 있습니다.

2.  **Full Fine-tuning -> 포토샵 전체 재설치**
    -   '인물 사진 보정' 기능을 추가하기 위해, 포토샵의 전체 소스 코드를 수정하고 수십 GB짜리 프로그램을 통째로 다시 설치하는 것과 같습니다. 매우 비효율적이고 저장 공간도 두 배로 차지합니다.

3.  **LoRA -> 포토샵 플러그인(Plugin)**
    -   포토샵 원본은 전혀 건드리지 않고, '인물 사진 보정' 기능만 담은 몇 MB짜리 가벼운 플러그인(LoRA 어댑터)을 만듭니다. 사용자는 필요할 때마다 이 플러그인을 활성화하여 사용하고, 다른 기능이 필요하면 다른 플러그인으로 갈아 끼울 수 있습니다.

---

#### 5. 실용적 관점에서의 LoRA 장점과 단점

**장점:**

-   **효율적인 학습**: 적은 GPU 메모리와 시간으로 파인튜닝이 가능합니다.
-   **효율적인 저장**: 과제별로 몇 MB 크기의 LoRA 어댑터만 저장하면 되므로 관리가 용이합니다.
-   **빠른 과제 전환**: 기본 모델 하나를 공유하며 필요에 따라 LoRA 어댑터만 교체(plug-and-play)할 수 있습니다.
-   **추론 지연 없음**: 학습 후 어댑터를 원본 가중치에 병합(`W' = W + BA`)하면, 추론 시에는 추가 계산이 전혀 없어 성능 저하가 없습니다.

**단점 및 고려사항:**

-   **성능의 한계**: 만약 과제가 매우 복잡하여 가중치의 'High-Rank' 변화가 필요한 경우, LoRA의 성능이 전체 파인튜닝에 비해 다소 낮을 수 있습니다.
-   **하이퍼파라미터**: 최적의 성능을 위해 `r` (rank), `lora_alpha` 등 추가적인 하이퍼파라미터 튜닝이 필요합니다.

> **결론적으로 LoRA의 단점은 "사용하면 안 되는 이유"라기보다는, "LoRA를 더 잘 사용하기 위해 이해하고 조절해야 할 부분"에 가깝습니다.**

---

#### 6. 다른 PEFT 기법들과의 비교

LoRA 외에도 다양한 PEFT 기법들이 존재하며, 각기 다른 철학을 가집니다.

-   **어댑터(Adapter)**: 모델 레이어 '사이'에 새로운 신경망 모듈을 '직렬'로 삽입하여 학습합니다. LoRA의 병렬 구조와 달리 추론 시 약간의 지연이 발생할 수 있습니다.
-   **프롬프트 튜닝(Prompt Tuning)**: 모델 가중치는 전혀 수정하지 않고, 입력 프롬프트의 일부를 구성하는 '가상 토큰' 벡터만 학습합니다.
    > **'학습 가능한 가상 토큰'이란?**
    > 
    > 모델의 진짜 입력은 단어가 아닌 '임베딩 벡터'라는 점에 착안한 아이디어입니다. 즉, 단어를 통해 고정된 벡터를 찾는 대신, 과제 수행에 가장 최적화된 **벡터 자체를 직접 학습**하는 것입니다. 이 벡터들은 어휘 사전에 없는 '가상'의 토큰에 해당하며, 모델의 다른 모든 가중치는 고정된 채 오직 이 '가상 토큰'의 벡터값만 학습 과정에서 업데이트됩니다. 모델에게 특정 과제를 푸는 방법을 알려주는 '마법의 주문'을 벡터 공간에서 직접 찾는 것과 같습니다.
-   **(IA)³**: 가중치가 아닌, 모델 내부의 활성화(Activation) 값을 조절하는 스케일링 팩터를 학습하여 정보의 흐름을 제어합니다.

---

#### 맺음말

LoRA는 파인튜닝의 효율성과 실용성을 크게 높인 인상적인 기법입니다. 모든 시나리오에서 최고의 성능을 보장하는 만능 해결책은 아닐지라도, 대부분의 경우 자원의 제약을 극복하고 원하는 모델을 만들 수 있는 강력한 도구가 되어줍니다.

물론 LoRA의 단점과 다른 PEFT 기법들의 존재도 명확히 인지하고, 해결하려는 과제의 특성에 맞는 최적의 방법을 선택하는 것이 중요할 것입니다. 저 또한 다음 단계로는 LoRA의 여러 변형이나 다른 기법들과의 실제 성능을 비교 분석하는 작업을 진행해볼 계획입니다.